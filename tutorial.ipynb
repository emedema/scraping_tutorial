{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6027ce55-1f4b-4417-a2f4-427647c5a6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Tutorial\n",
    "\n",
    "This Scraping Tutorial created for Tracking Injustice to aid in the creation of a living dataset tracking Canadian Police-Involved Deaths. For more information regarding Tracking Injustice, see https://trackinginjustice.ca/.\n",
    "\n",
    "This tutorial is run and created by Emily Medema and Rohan Khan.\n",
    "\n",
    "## Interactive Scraping Example\n",
    "\n",
    "The best way to learn how to scrape is to do it. As we have seen in the slides (which can be found here: [Slide Link](\"http://tiny.cc/scraping-tutorial-slides\")), websites differ greatly and therefore our scraping techniques have to be customized to the site. The best way to learn to do that, while following the guidelines also laid out in the slides, is to do it yourself. Scraping is also a continuous process of creating and maintainence. A website may change which can cause your script to become ineffective. You must be able to adapt so that your script can continue to be effective.\n",
    "\n",
    "First, we will get our environment setup to scrape and then we will work our way up from simply examples to more and more complex ones. \n",
    "\n",
    "### Environmental Setup\n",
    "\n",
    "We are using Jupyter Notebooks, which is an open source web application that you can use to create and share documents that contain live code, equations, visualizations, and text. It is incredibly useful for Python. Through jupyter notebooks you can create a document that documents, explains, and contains your code all in one place. This is very helpful for maintainence purposes.\n",
    "\n",
    "Jupyter Notebooks run from either third-party sites, your virtual environment, or your localhost. Therefore, you can access whatever python libaries installed either on that third-party site, your virtual environment, or your own machine by simply importing the libary. If you ever have to install a libary on the notebook itself you can do so with this command:\n",
    "\n",
    "```\n",
    "!pip install libaryname\n",
    "```\n",
    "\n",
    "For this scraping tutorial, we will want the following libraries:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- urllib3\n",
    "- bs4\n",
    "- MechanicalSoup\n",
    "- Scrapy\n",
    "- selenium\n",
    "\n",
    "You will most likely never need all of these for scraping. Nevertheless, ensure that all these are installed on whatever machine you are running jupyter notebook on.\n",
    "\n",
    "You can easily install all of these libraries if you have the [github repository](\"https://github.com/emedema/scraping_tutorial\") cloned by running the following command:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Now that we have all these libraries installed, we can import them into the notebook. This means we can then use their methods etc. within our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3c75bd-b612-41f8-bed4-6039d6537214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import mechanicalsoup\n",
    "import scrapy\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3c0dd-c210-4da5-ab67-aae68d5fd18f",
   "metadata": {},
   "source": [
    "Now that we have our environment setup, we can move onto the first scraping exercise.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "For our first exercise, we are going to scrape this site: http://olympus.realpython.org/profiles/aphrodite\n",
    "\n",
    "This was setup as a site for building your first webscraper, so it is a good first step.\n",
    "\n",
    "On the site, we can see that there is a few different pieces of information:\n",
    "\n",
    "1. There is an image of Aphrodite\n",
    "2. Her name is shown in an H2 tag like as \"Name: Aphrodite\"\n",
    "3. Other information is also shown on the page within the \\<center> tag\n",
    "\n",
    "To grab this information, we are going to use the `urllib` library to grab the source code for the site and then parse that code to get the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de538071-8d8d-470d-9ee3-4fac8955cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x17df5ff70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we specifically want the urlopen method from urllib.request\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# assign the url to a variable for ease\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "\n",
    "# to open a webpage pass the url to urlopen\n",
    "page = urlopen(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14aedd-a636-420e-b78e-49c0ffe378bb",
   "metadata": {},
   "source": [
    "`urlopen` returns an HTTPResponse object, therefore we need to first use the HTTPResponse object’s `.read()` method, which returns a sequence of bytes. Then use `.decode()` to decode the bytes to a string using UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfa698f-52b2-411b-a884-b3f395f8eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Profile: Aphrodite</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/aphrodite.gif\" />\n",
      "<h2>Name: Aphrodite</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dove\n",
      "<br><br>\n",
      "Favorite color: Red\n",
      "<br><br>\n",
      "Hometown: Mount Olympus\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_bytes = page.read()\n",
    "html = html_bytes.decode(\"utf-8\")\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332e02d-93ed-4477-8a4b-d740e3ce585d",
   "metadata": {},
   "source": [
    "Now that we have the text of the HTML, we could extract the information using Python's string methods.\n",
    "\n",
    "We could use the `.find()` method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9248f552-2ef5-4fdf-b659-da1b7dc0c698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Profile: Aphrodite'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = html.find(\"<title>\") + len(\"<title>\")\n",
    "end_index = html.find(\"</title>\")\n",
    "title = html[start_index:end_index]\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a581b53-cf15-49f7-b68c-ae651bcfda5d",
   "metadata": {},
   "source": [
    "But this is not reliable as HTML can change drastically between sites and can be very messy. A simple change such as going from `<title>` to `<title >` can result in a logic error in our scraping despite it not effecting the site itself.\n",
    "\n",
    "Therefore, this is not the best way to utilize the string methods of python. A better way would be to use Regular Expressions or regex. Regex are patterns that you can use to search for text within a string. Python supports regular expressions through the standard library’s re module, which we have already imported. \n",
    "\n",
    "Regular expressions use special characters called metacharacters to denote different patterns. For instance, the asterisk character (*) stands for zero or more instances of whatever comes just before the asterisk.\n",
    "\n",
    "In the following example, you use `.findall()` to find any text within a string that matches a given regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72bfcc3-d6e1-437f-8fbb-9fac4fc0547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"ac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e4e7b-1661-4fd8-a423-5a6415978df0",
   "metadata": {},
   "source": [
    "The first argument of `re.findall()` is the regular expression that you want to match, and the second argument is the string to test. In the above example, you search for the pattern \"ab*c\" in the string \"ac\".\n",
    "\n",
    "The regular expression \"ab*c\" matches any part of the string that begins with \"a\", ends with \"c\", and has zero or more instances of \"b\" between the two. `re.findall()` returns a list of all matches. The string \"ac\" matches this pattern, so it’s returned in the list.\n",
    "\n",
    "Here’s the same pattern applied to different strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63740180-4707-460d-ae3c-723eee2e681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc']\n",
      "['ac']\n",
      "['abc', 'ac']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"ab*c\", \"abcd\"))\n",
    "print(re.findall(\"ab*c\", \"acc\"))\n",
    "print(re.findall(\"ab*c\", \"abcac\"))\n",
    "print(re.findall(\"ab*c\", \"abdc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5117e-6113-4a65-86c9-e019891b4606",
   "metadata": {},
   "source": [
    "Notice that if no match is found, then `.findall()` returns an empty list.\n",
    "\n",
    "Pattern matching is case sensitive. If you want to match this pattern regardless of the case, then you can pass a third argument with the value `re.IGNORECASE`.\n",
    "\n",
    "You can use a period (.) to stand for any single character in a regular expression. For instance, you could find all the strings that contain the letters \"a\" and \"c\" separated by a single character as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9885010-d1bc-499a-9ee3-1d93316f5b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"a.c\", \"abc\"))\n",
    "print(re.findall(\"a.c\", \"abbc\"))\n",
    "print(re.findall(\"a.c\", \"ac\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3342d-8560-4d84-bd32-8026c7cf5bde",
   "metadata": {},
   "source": [
    "The pattern .* inside a regular expression stands for any character repeated any number of times. For instance, you can use \"a.*c\" to find every substring that starts with \"a\" and ends with \"c\", regardless of which letter—or letters—are in between\n",
    "\n",
    "Often, you use `re.search()` to search for a particular pattern inside a string. This function is somewhat more complicated than `re.findall()` because it returns an object called MatchObject that stores different groups of data. This is because there might be matches inside other matches, and `re.search()` returns every possible result.\n",
    "\n",
    "The details of MatchObject are irrelevant here. For now, just know that calling `.group()` on MatchObject will return the first and most inclusive result, which in most cases is just what you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d80fc25-b8ea-4930-ad7f-9934c4ceeda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABC'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_results = re.search(\"ab*c\", \"ABC\", re.IGNORECASE)\n",
    "match_results.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e8431-e636-49fe-9049-6e8bfeeaadb8",
   "metadata": {},
   "source": [
    "There’s one more function in the `re` module that’s useful for parsing out text. `re.sub()`, which is short for substitute, allows you to replace the text in a string that matches a regular expression with new text. It behaves sort of like the `.replace()` string method.\n",
    "\n",
    "The arguments passed to `re.sub()` are the regular expression, followed by the replacement text, followed by the string. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad004ab-fb71-45bd-890b-db189b1fdc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everything is ELEPHANTS.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*>\", \"ELEPHANTS\", string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832ed96-9749-4d85-9037-997420c21c51",
   "metadata": {},
   "source": [
    "But as we can see, we need to be careful with regex as we might accidentally do something like this. \n",
    "\n",
    "`re.sub()` uses the regular expression \"<.*>\" to find and replace everything between the first < and the last >, which spans from the beginning of \\<replaced> to the end of \\<tags>. This is because Python’s regular expressions are greedy, meaning they try to find the longest possible match when characters like * are used.\n",
    "\n",
    "Alternatively, you can use the non-greedy matching pattern *?, which works the same way as * except that it matches the shortest possible string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d469950f-13fc-4ebe-9dea-30ee848b1d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Everything is ELEPHANTS if it's in ELEPHANTS.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*?>\", \"ELEPHANTS\", string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ca09c-0663-49e5-b377-e61585465404",
   "metadata": {},
   "source": [
    "This time, `re.sub()` finds two matches, <replaced> and <tags>, and substitutes the string \"ELEPHANTS\" for both matches.\n",
    "    \n",
    "Now let's try this on our Aphrodite page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd3c5877-8793-47aa-af35-98b937ab08e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Aphrodite\n"
     ]
    }
   ],
   "source": [
    "# regex pattern, searching for the text between the title tags\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "# Remove HTML tags\n",
    "title = re.sub(\"<.*?>\", \"\", title)\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039dfb8-a5eb-437f-885a-68b10b2741d4",
   "metadata": {},
   "source": [
    "Let's try it on another site with messier HTML, such as this one: http://olympus.realpython.org/profiles/dionysus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d154b89-d3fa-4056-927f-a3d76c213d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Dionysus\n"
     ]
    }
   ],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "title = re.sub(\"<.*?>\", \"\", title) # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb02c56-56f3-4a2f-aac0-35cfb7d42ca7",
   "metadata": {},
   "source": [
    "Let's take a closer look at the first regular expression in the pattern string by breaking it down into three parts:\n",
    "\n",
    "1. <title.*?> matches the opening \\<TITLE > tag in html. The \\<title part of the pattern matches with \\<TITLE because re.search() is called with re.IGNORECASE, and .*?> matches any text after \\<TITLE up to the first instance of >.\n",
    "\n",
    "2. .*? non-greedily matches all text after the opening \\<TITLE >, stopping at the first match for \\</title.*?>.\n",
    "\n",
    "3. \\</title.*?> differs from the first pattern only in its use of the / character, so it matches the closing \\</title  / > tag in html.\n",
    "\n",
    "The second regular expression, the string \"<.*?>\", also uses the non-greedy .*? to match all the HTML tags in the title string. By replacing any matches with \"\", `re.sub()` removes all the tags and returns only the text.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Although regular expressions are great for pattern matching in general, sometimes it’s easier to use an HTML parser that’s explicitly designed for parsing out HTML pages. There are many Python tools written for this purpose, but the Beautiful Soup library is a good one to start with.\n",
    "\n",
    "To use `BeautifulSoup`, we will create a program that does the following:\n",
    "\n",
    "   1. Opens the URL http://olympus.realpython.org/profiles/dionysus by using `urlopen()` from the urllib.request module\n",
    "\n",
    "   2. Reads the HTML from the page as a string and assigns it to the html variable\n",
    "\n",
    "   3. Creates a BeautifulSoup object and assigns it to the soup variable\n",
    "\n",
    "The `BeautifulSoup` object assigned to soup is created with two arguments. The first argument is the HTML to be parsed, and the second argument, the string \"html.parser\", tells the object which parser to use behind the scenes. \"html.parser\" represents Python’s built-in HTML parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "009ee9cf-a3aa-4aa5-aff0-cfdc73f33fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7f4d8-6170-4e1b-ad3c-b16d0fb61919",
   "metadata": {},
   "source": [
    "Now that we have the HTML saved and parsed in a soup object, we can use the soup methods such as `get_text()` that you can use to extract all the text from the document and automatically remove any HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d0a117-0489-4b4b-94db-0a6b7ec4c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profile: Dionysus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Dionysus\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "Favorite animal: Leopard \n",
      "\n",
      "Favorite Color: Wine\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cad3d6-4a19-4147-8506-947d0a87b3f4",
   "metadata": {},
   "source": [
    "There are a lot of blank lines in this output. These are the result of newline characters in the HTML document’s text. You can remove them with the `.replace()` string method if you need to.\n",
    "\n",
    "Often, you need to get only specific text from an HTML document. Using Beautiful Soup first to extract the text and then using the `.find()` string method is sometimes easier than working with regular expressions.\n",
    "\n",
    "However, other times the HTML tags themselves are the elements that point out the data you want to retrieve. For instance, perhaps you want to retrieve the URLs for all the images on the page. These links are contained in the src attribute of \\<img> HTML tags.\n",
    "\n",
    "In this case, you can use `find_all()` to return a list of all instances of that particular tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6a3b93a-ba8c-4a50-b08a-718d3bfe8f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img src=\"/static/dionysus.jpg\"/>, <img src=\"/static/grapes.png\"/>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf271b-b3cb-4132-963f-769c5c6cc7b6",
   "metadata": {},
   "source": [
    "This returns a list of all \\<img> tags in the HTML document. The objects in the list look like they might be strings representing the tags, but they’re actually instances of the Tag object provided by Beautiful Soup. Tag objects provide a simple interface for working with the information they contain.\n",
    "\n",
    "You can explore this a little by first unpacking the Tag objects from the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9388f738-d506-4491-851d-8619416dbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, image2 = soup.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd93c7-11af-4466-a36e-819ac52518d2",
   "metadata": {},
   "source": [
    "Each Tag object has a .name property that returns a string containing the HTML tag type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f86692c-199f-4e97-b64d-b47de9c395af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605a50c-cb90-412e-a94e-114c88274090",
   "metadata": {},
   "source": [
    "You can access the HTML attributes of the Tag object by putting their names between square brackets, just as if the attributes were keys in a dictionary.\n",
    "\n",
    "For example, the `<img src=\"/static/dionysus.jpg\"/>` tag has a single attribute, src, with the value \"/static/dionysus.jpg\". Likewise, an HTML tag such as the link `<a href=\"https://realpython.com\" target=\"_blank\">` has two attributes, href and target.\n",
    "\n",
    "To get the source of the images in the Dionysus profile page, you access the src attribute using the dictionary notation mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb134cde-c114-4563-8657-340bdee45cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/static/grapes.png'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image2[\"src\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d16885-9a52-45bb-a76c-21c6e904d58f",
   "metadata": {},
   "source": [
    "You can also access certain tags from the HTML page such as title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19627084-85d9-4ca5-b654-a31c85822c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Profile: Dionysus</title>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06722fa7-0817-479d-9e63-feb90868f51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Profile: Dionysus'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691e69a-75d4-4f49-9a0a-7f8ad29cf7ee",
   "metadata": {},
   "source": [
    "One of the features of Beautiful Soup is the ability to search for specific kinds of tags whose attributes match certain values. For example, if you want to find all the \\<img> tags that have a src attribute equal to the value /static/dionysus.jpg, then you can provide the following additional argument to `.find_all()`.\n",
    "\n",
    "This example is somewhat arbitrary, and the usefulness of this technique may not be apparent from the example. If you spend some time browsing various websites and viewing their page sources, then you’ll notice that many websites have extremely complicated HTML structures.\n",
    "\n",
    "When scraping data from websites with Python, you’re often interested in particular parts of the page. By spending some time looking through the HTML document, you can identify tags with unique attributes that you can use to extract the data you need.\n",
    "\n",
    "Then, instead of relying on complicated regular expressions or using .find() to search through the document, you can directly access the particular tag that you’re interested in and extract the data you need.\n",
    "\n",
    "In some cases, you may find that Beautiful Soup doesn’t offer the functionality you need. The `lxml` library is somewhat trickier to get started with but offers far more flexibility than Beautiful Soup for parsing HTML documents. You may want to check it out once you’re comfortable using Beautiful Soup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c8e9b-038f-4c8d-a824-d031ac6b619d",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Beautiful Soup is great for scraping data from a website’s HTML, but it doesn’t provide any way to work with HTML forms. For example, if you need to search a website for some query and then scrape the results, then Beautiful Soup alone won’t get you very far. In this case, we can use `MechanicalSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19004244-612f-4e05-93cb-3bacfd2fd631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
