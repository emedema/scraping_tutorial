{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6027ce55-1f4b-4417-a2f4-427647c5a6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Tutorial\n",
    "\n",
    "This Scraping Tutorial created for Tracking Injustice to aid in the creation of a living dataset tracking Canadian Police-Involved Deaths. For more information regarding Tracking Injustice, see https://trackinginjustice.ca/.\n",
    "\n",
    "This tutorial is run and created by Emily Medema and Rohan Khan.\n",
    "\n",
    "## Interactive Scraping Example\n",
    "\n",
    "The best way to learn how to scrape is to do it. As we have seen in the slides (which can be found here: [Slide Link](\"http://tiny.cc/scraping-tutorial-slides\")), websites differ greatly and therefore our scraping techniques have to be customized to the site. The best way to learn to do that, while following the guidelines also laid out in the slides, is to do it yourself. Scraping is also a continuous process of creating and maintainence. A website may change which can cause your script to become ineffective. You must be able to adapt so that your script can continue to be effective.\n",
    "\n",
    "First, we will get our environment setup to scrape and then we will work our way up from simply examples to more and more complex ones. \n",
    "\n",
    "### Environmental Setup\n",
    "\n",
    "We are using Jupyter Notebooks, which is an open source web application that you can use to create and share documents that contain live code, equations, visualizations, and text. It is incredibly useful for Python. Through jupyter notebooks you can create a document that documents, explains, and contains your code all in one place. This is very helpful for maintainence purposes.\n",
    "\n",
    "Jupyter Notebooks run from either third-party sites, your virtual environment, or your localhost. Therefore, you can access whatever python libaries installed either on that third-party site, your virtual environment, or your own machine by simply importing the libary. If you ever have to install a libary on the notebook itself you can do so with this command:\n",
    "\n",
    "```\n",
    "!pip install libaryname\n",
    "```\n",
    "\n",
    "For this scraping tutorial, we will want the following libraries:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- urllib3\n",
    "- bs4\n",
    "- MechanicalSoup\n",
    "- Scrapy\n",
    "- selenium\n",
    "\n",
    "You will most likely never need all of these for scraping. Nevertheless, ensure that all these are installed on whatever machine you are running jupyter notebook on.\n",
    "\n",
    "You can easily install all of these libraries if you have the [github repository](\"https://github.com/emedema/scraping_tutorial\") cloned by running the following command:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Now that we have all these libraries installed, we can import them into the notebook. This means we can then use their methods etc. within our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3c75bd-b612-41f8-bed4-6039d6537214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import mechanicalsoup\n",
    "import scrapy\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6bc64b-b361-4fd7-b51f-fe4f8ee865f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
